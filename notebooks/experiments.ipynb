{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "I will use GPT2 which is free."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "['.git',\n '.gitignore',\n '.idea',\n 'data',\n 'experiments',\n 'fine_tuned',\n 'notebooks',\n 'out',\n 'README.md',\n 'shakespeare_data',\n 'utils.py',\n '__pycache__']"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('..')\n",
    "os.listdir()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GIGABYTE\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (None)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils import get_model_tokenizer\n",
    "\n",
    "model_raw_name = 'gpt2'\n",
    "model, tokenizer = get_model_tokenizer(model_raw_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GIGABYTE\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "data = 'data/enhanced'\n",
    "train_file = os.path.join(data, \"train.txt\")\n",
    "test_file = os.path.join(data, \"test.txt\")\n",
    "\n",
    "\n",
    "data_col = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "eval_dataset = TextDataset(tokenizer=tokenizer, file_path=test_file, block_size=1024)\n",
    "train_dataset = TextDataset(tokenizer=tokenizer, file_path=train_file, block_size=1024)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "output_dir = 'experiments'\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs = 5,\n",
    "    output_dir = output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    per_device_train_batch_size = 8\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = train_args,\n",
    "    data_collator = data_col,\n",
    "    eval_dataset = eval_dataset,\n",
    "    train_dataset = train_dataset\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard is running at http://localhost:6666/\n"
     ]
    }
   ],
   "source": [
    "from utils import launch_tensorboard\n",
    "\n",
    "launch_tensorboard(log_dir='experiments', port=6666)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GIGABYTE\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 298\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 190\n",
      "  Number of trainable parameters = 124439808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/190 : < :, Epoch 0.03/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=190, training_loss=3.4696311549136514, metrics={'train_runtime': 350.4479, 'train_samples_per_second': 4.252, 'train_steps_per_second': 0.542, 'total_flos': 778650255360000.0, 'train_loss': 3.4696311549136514, 'epoch': 5.0})\n"
     ]
    }
   ],
   "source": [
    "print(model.device)\n",
    "out = trainer.train()\n",
    "if out:\n",
    "    print(out)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to experiments\n",
      "Configuration saved in experiments\\config.json\n",
      "Configuration saved in experiments\\generation_config.json\n",
      "Model weights saved in experiments\\pytorch_model.bin\n",
      "tokenizer config file saved in experiments\\tokenizer_config.json\n",
      "Special tokens file saved in experiments\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": "('experiments\\\\tokenizer_config.json',\n 'experiments\\\\special_tokens_map.json',\n 'experiments\\\\vocab.json',\n 'experiments\\\\merges.txt',\n 'experiments\\\\added_tokens.json')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from utils import get_model_tokenizer\n",
    "\n",
    "inference_model_name = \"../experiments\"\n",
    "inference_model, inference_tokenizer = get_model_tokenizer(inference_model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love or Hate, that is the question:\n",
      "And if you have not, you are not a man\n",
      "To be hated.\n",
      "\n",
      "KING RICHARD II: I do not know, sir, but I know\n",
      "That you do hate me. I have been a prisoner\n",
      "Of your tyranny, and I am a traitor to you. You\n",
      "Have made me a slave to your power,\n",
      "For I did not love you, nor did I love\n",
      "Your majesty. But I will not be a king, for\n",
      "I am not your king. Therefore, I'll be\n",
      "A king to the people. If you will, then I shall be king\n",
      "In the land of England. For I hate you\n",
      "More than you hate yourself. What, my lord, is this?\n",
      "What, what, this is a lie? What is it? I\n",
      "Am a prince, a nobleman, an honest man,--\n",
      "The king of France, the king and queen of\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import  generate_text\n",
    "\n",
    "input_text = \"Love or Hate, that is the question:\"\n",
    "generated_text = generate_text(inference_model, inference_tokenizer, input_text, max_length=200)\n",
    "print(generated_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My country is under attack, fight or die, believe or hope?\n",
      "\n",
      "KING RICHARD II:\n",
      "I have heard the news of the king's death.\n",
      "The king is dead, and the queen is in the womb. I\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import  generate_text\n",
    "\n",
    "input_text = \"My country is under attack, fight or die, believe or hope?\"\n",
    "generated_text = generate_text(inference_model, inference_tokenizer, input_text, max_length=50)\n",
    "print(generated_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tf_gpu",
   "language": "python",
   "display_name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
